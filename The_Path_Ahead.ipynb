{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEmBz7aMpPN7v08DOMOn5s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kilos11/Learn-Keras-for-Deep-Neural-Networks-by-JoJo-Moolayil/blob/main/The_Path_Ahead.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CNN**\n",
        "\n",
        "CNNs are the class of DL algorithms used for computer vision use cases\n",
        "like classifying an image or a video and detecting an object within an\n",
        "image or even a region within an image. CNN algorithms were a huge\n",
        "breakthrough in the field of computer vision, as it required a bare\n",
        "minimum of image processing compared to the other prevalent techniques\n",
        "of the time and also performed exceptionally well. The performance\n",
        "improvement with CNN for image classification was phenomenal. The\n",
        "process of building CNN is also simplified in Keras, where all the logical\n",
        "components are neatly abstracted. Keras provides CNN layers, and the\n",
        "overall process of developing CNN models is quite similar to what we\n",
        "learned while developing regression and classification models.\n",
        "To give a brief understanding of the process, we will use a small\n",
        "example with its implementation. The following code snippet showcases a\n",
        "‘hello world’ equivalent implementation for CNN. We will use the MNIST\n",
        "data (i.e., a collection of images with handwritten digits). The objective\n",
        "would be to classify the image as one of the digits from [0,1,2,3,4,5,6,7,8,9].\n",
        "The data is already available in the Keras dataset module. Though the topic  is entirely new, the comments within the code snippet will provide you\n",
        "with a basic idea of the model design."
      ],
      "metadata": {
        "id": "acgNA_WUhZvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Fu9fnvyiS5d",
        "outputId": "5c9b79af-07af-477a-eb5d-9a17e4f3e50d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.7)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtFqUv9ThAwO"
      },
      "outputs": [],
      "source": [
        "#Importing the necessary packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "#Importing the CNN related layers as described in Chapter 2\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "#Printing the Keras version\n",
        "print(\"Keras version:\", tensorflow.keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading data from Keras datasets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "#Defining the height and weight and number of samples\n",
        "#Each Image is a 28 x 28 with 1 channel matrix\n",
        "training_samples, height, width = x_train.shape\n",
        "testing_samples,_,_ = x_test.shape\n",
        "print(\"Training Samples:\",training_samples)\n",
        "print(\"Testing Samples:\",testing_samples)\n",
        "print(\"Height: \"+str(height)+\" x Width:\"+ str(width))"
      ],
      "metadata": {
        "id": "hEeBNZgqjplm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets have a look at a sample image in the training data\n",
        "plt.imshow(x_train[0],cmap='gray', interpolation='none')\n",
        "#We now have to engineer the image data into the right form\n",
        "#For CNN, we would need the data in Height x Width X Channels\n",
        "#form Since the image is in grayscale, we will use channel = 1\n",
        "channel =1\n",
        "x_train = x_train.reshape(training_samples, height,\n",
        "width,channel).astype('float32')\n",
        "x_test = x_test.reshape(testing_samples, height, width,\n",
        "channel).astype('float32')\n",
        "#To improve the training process, we would need to standardize\n",
        "#or normalize the values We can achieve this using a simple\n",
        "#divide by 256 for all values\n",
        "x_train = x_train/255\n",
        "x_test =x_test/255\n",
        "#Total number of digits =10\n",
        "target_classes = 10\n",
        "# numbers 0-9, so ten classes\n",
        "n_classes = 10\n",
        "# convert integer labels into one-hot vectors\n",
        "y_train = np_utils.to_categorical(y_train, n_classes)\n",
        "y_test = np_utils.to_categorical(y_test, n_classes)"
      ],
      "metadata": {
        "id": "cyxBeYhtj7Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Designing the CNN Model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (5, 5), input_shape=(height,width ,1),\n",
        "activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "model.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
        "epochs=10, batch_size=200)"
      ],
      "metadata": {
        "id": "CNoCLMVHkxm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finally, let’s evaluate the model performance:\n",
        "metrics = model.evaluate(x_test, y_test, verbose=0)\n",
        "for i in range(0,len(model.metrics_names)):\n",
        " print(str(model.metrics_names[i])+\" = \"+str(metrics[i]))"
      ],
      "metadata": {
        "id": "IblklIBSlbyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**RNN**\n",
        "The next step in DL after having explored CNN is to start exploring RNN,\n",
        "popularly known as “sequence models.” This name became popular\n",
        "because RNN makes use of sequential information. So far, all the DNNs\n",
        "that we have explored process training data with the assumption that\n",
        "there is no relationship between any two training samples. However, this\n",
        "is an issue for many problems that we can solve using data. Consider\n",
        "the predictive text feature in your iOS or Android phone; the prediction\n",
        "of the next word is highly dependent on the last few words you already\n",
        "typed. That’s where the sequential model comes into the picture. RNNs\n",
        "can also be understood as neural networks with memory. It connects\n",
        "a layer to itself and thereby gets simultaneous access to two or more\n",
        "consecutive input samples to process the end output. This property is\n",
        "unique to RNN, and with its rise in research, it delivered amazing success\n",
        "in the field of natural language understanding.\n",
        " All the legacy natural\n",
        "language processing techniques could now be significantly improved\n",
        "with RNNs. The rise of chatbots, improved autocorrect in text messaging,\n",
        "suggested reply in e-mail clients and other apps, and machine translation\n",
        "(i.e., translating text from a source language to a target language, Google\n",
        "Translate being the classic example) have all been propelled with the\n",
        "adoption of RNN. There are again different types of LSTM (long short-term\n",
        "memory) networks that overcome the limitations within the existing RNN\n",
        "architecture and take performance for natural language processing–related\n",
        "tasks a notch higher. The most popular versions of RNN are LSTM and\n",
        "GRU (gated recurrent unit) networks.\n",
        "Similar to what we did for CNN, we will have a look at a simple (hello\n",
        "world equivalent) sample implementation for RNN/LSTM networks.\n",
        "The following code snippet performs a binary classification on the IMDB\n",
        "reviews dataset within Keras. It is a use case where we are provided with\n",
        "user reviews (text date) and an associated outcome as Positive or Negative."
      ],
      "metadata": {
        "id": "NgyWF6rZmhCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the necessary packages\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence"
      ],
      "metadata": {
        "id": "tNwu2Jgym6kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting a max cap for the number of distinct words\n",
        "top_words = 5000\n",
        "#Loading the training and test data from keras datasets\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "\n",
        "#Since the length of each text will be varying\n",
        "#We will pad the sequences (i.e. text) to get a uniform length\n",
        "#throughout\n",
        "max_text_length = 500\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_text_length)\n",
        "\n"
      ],
      "metadata": {
        "id": "yohLzxIQnHLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Design the network\n",
        "embedding_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_length,\n",
        "input_length=max_text_length))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "OWFx50LPsKEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
        "metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "q3A-SZHHsTFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit the model\n",
        "model.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
        "epochs=3, batch_size=64)"
      ],
      "metadata": {
        "id": "IWrup5aBshfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the accuracy on the test dataset:\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Accuracy:\",scores[1])"
      ],
      "metadata": {
        "id": "B6qktkk2sxHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CNN + RNN**\n",
        "\n",
        "Another interesting area to explore within DL is the intersection of CNN\n",
        "and RNN. Sounds confusing? Just imagine you could combine the power\n",
        "of CNN (i.e., understanding images) and that of RNN (i.e., understanding\n",
        "natural text); what could the intersection or combination look like? You\n",
        "could describe a picture with words. That’s right, by combining RNN and\n",
        "CNN together, we could help computers describe an image with natural￾style text. The process is called image captioning. Today, if you search on\n",
        "google.com, a query like “yellow cars,” your results will actually return a\n",
        "ton of yellow cars. If you imagine that the captioning for these images was\n",
        "done by humans, which could then be indexed by search engines, you are\n",
        "absolutely wrong.With humans, we can’t scale the process of captioning\n",
        "images to billions of images per day. The process is simply not viable. You\n",
        "would need a smarter way to do that. Image captioning with CNN+RNN\n",
        "has brought a breakthrough not only in an image search for search engines\n",
        "but several other products we use in our day-to-day lives. The most important and revolutionary outcome that was delivered to mankind by\n",
        "the intersection of RNN and CNN was smart glasses (called duLight by\n",
        "Baidu): a camera equipped to reading glasses that could describe what the\n",
        "surroundings looked like. This was a great product for visually impaired\n",
        "people. Today, we have a smaller version of that implemented in a few\n",
        "apps that can be installed on the phone and works with the phone camera.  "
      ],
      "metadata": {
        "id": "8V96GHSrttFT"
      }
    }
  ]
}