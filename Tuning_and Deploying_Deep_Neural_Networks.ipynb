{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kilos11/Learn-Keras-for-Deep-Neural-Networks-by-JoJo-Moolayil/blob/main/Tuning_and%C2%A0Deploying_Deep_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmMxdwcG3RSQ"
      },
      "source": [
        "#**L1 Regularization**\n",
        "\n",
        "In L1 regularization, the absolute weights are added to the loss function. To\n",
        "make the model more generalized, the values of the weights are reduced\n",
        "to 0, and therefore this method is strongly preferred when we are trying to\n",
        "compress the model for faster computation.\n",
        "In Keras, the L1 loss can be added to a layer by providing the ‘regularizer’\n",
        "object to the ‘kernel regularizer’ parameter. The following code snippet\n",
        "demonstrates adding an L1 regularizer to a dense layer in Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdhgOO_ZKY-x"
      },
      "outputs": [],
      "source": [
        "from keras import regularizers\n",
        "from keras import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(256, input_dim=128,\n",
        "kernel_regularizer=regularizers.l1(0.01)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5TxjVLF3qzw"
      },
      "source": [
        "#**L2 Regularization**\n",
        "\n",
        "In L2 regularization, the squared weights are added to the loss function. To\n",
        "make the model more generalized, the values of the weights are reduced to\n",
        "near 0 (but not actually 0), and hence this is also called the “weight decay”\n",
        "method. In most cases, L2 is highly recommended over L1 for reducing\n",
        "overfitting.\n",
        "L2 Regularization\n",
        "In L2 regularization, the squared weights are added to the loss function. To\n",
        "make the model more generalized, the values of the weights are reduced to\n",
        "near 0 (but not actually 0), and hence this is also called the “weight decay”\n",
        "method. In most cases, L2 is highly recommended over L1 for reducing\n",
        "overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lK359UPY4BRe"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(256, input_dim=128,\n",
        "kernel_regularizer=regularizers.l2(0.01)))\n",
        "#The value of 0.01 is the hyperparameter value we set for λ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyTIQ1uH4Y5V"
      },
      "source": [
        "#**Dropout Regularization**\n",
        "\n",
        "\n",
        "*In addition to L1 and L2 regularization, there is another popular technique\n",
        "in DL to reduce overfitting. This technique is to use a dropout mechanism.\n",
        "In this method, the model arbitrarily drops or deactivates a few neurons\n",
        "for a layer during each iteration. Therefore, in each iteration the model\n",
        "looks at a slightly different structure of itself to optimize (as a couple\n",
        "of neurons and the connections would be deactivated). Say we have\n",
        "two successive layers, H1 and H2, with 15 and 20 neurons, respectively.\n",
        "Applying the dropout technique between these two layers would result\n",
        "in randomly dropping a few neurons (based on a defined percentage) for\n",
        "H1, which therefore reduces the connections between H1 and H2. This\n",
        "process repeats for each iteration with randomness, so if the model has\n",
        "learned for a batch and updated the weights, the next batch might have a\n",
        "fairly different set of weights and connections to train.\n",
        " The process is not\n",
        "only efficient due to the reduced computation but also works intuitively in\n",
        "reducing the overfitting and therefore improving the overall performance.\n",
        "The idea of dropout can be visually understood using the following\n",
        "figure. We can see that the regular network has all neurons and connections\n",
        "between two successive layers intact. With dropout, each iteration induces a\n",
        "certain defined degree of randomness by arbitrarily deactivating or dropping\n",
        "a few neurons and their associated weight connections.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEVUQpMS49GM"
      },
      "outputs": [],
      "source": [
        "from keras import Sequential\n",
        "from keras.layers.core import Dropout, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(100, input_dim= 50, activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(1,activation=\"linear\"))\n",
        "\n",
        "#The parameter value of 0.25 indicates the dropout rate\n",
        "#(i.e., the percentage of the neurons to be dropped)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCrtw8wq5gqy"
      },
      "source": [
        "#**Hyperparameter Tuning**\n",
        "\n",
        "Hyperparameters are the parameters that define a model’s\n",
        "holistic structure and thus the learning process. We can also relate\n",
        "hyperparameters as the metaparameter for a model. It differs from a\n",
        "model’s actual parameters, which it learns during the training process (say,\n",
        "the model weights). Unlike model parameters, hyperparameters cannot be\n",
        "learned; we need to tune them with different approaches to get improved\n",
        "performance.\n",
        "\n",
        "\n",
        "\n",
        "###**Number of Neurons in a Layer**\n",
        "\n",
        "For most classification and regression use cases using tabular cross￾sectional data, DNNs can be made robust by playing around with the\n",
        "width of the network (i.e., the number of neurons in a layer). Generally,\n",
        "a simple rule of thumb for selecting the number of neurons in the first\n",
        "layer is to refer to the number of input dimensions. If the final number of\n",
        "input dimensions in a given training dataset (this includes the one-hot encoded features also) is x, we should use at least the closest number to 2x\n",
        "in the power of 2. Let’s say you have 100 input dimensions in your training\n",
        "dataset: preferably start with 2 × 100 = 200, and take the closest power of 2,\n",
        "so 256. It is good to have the number of neurons in the power of 2, as it\n",
        "helps the computation of the network to be faster. Also, good choices for\n",
        "the number of neurons would be 8, 16, 32, 64, 128, 256, 512, 1024, and so\n",
        "on. Based on the number of input dimensions, take the number closest to\n",
        "2 times the size. So, when you have 300 input dimensions, try using 512\n",
        "neurons.\n",
        "\n",
        "\n",
        "\n",
        "###**Number of Layers**\n",
        "\n",
        "It is true that just adding a few more layers will generally increase the\n",
        "performance, at least marginally. But the problem is that with an increased\n",
        "number of layers, the training time and computation increase significantly.\n",
        "Moreover, you would need a higher number of epochs to see promising\n",
        "results. Not using deeper networks is not an always an option; in cases\n",
        "when you have to, try using a few best practices.\n",
        "In case you are using a very large network, say more than 20 layers,\n",
        "try using a tapering size architecture (i.e., gradually reduce the number\n",
        "of neurons in each layer as the depth increases). So, if you are using an\n",
        "architecture of 30 layers with 512 neurons in each layer, try reducing the\n",
        "number of neurons in the layers slowly. An improved architecture would\n",
        "be with the first 8 layers having 512 neurons, the next 8 with 256, the next\n",
        "8 with 128, and so on. For the last hidden layer (not the output layer), try keeping the number of neurons to at least around 30–40% of the input size.\n",
        "Alternatively, if you are using wider networks (i.e., not reducing the\n",
        "number of neurons in the lower layers), always use L2 regularization or\n",
        "dropout layers with a drop rate of around 30%. The chances of overfitting\n",
        "are highly reduced.\n",
        "\n",
        "\n",
        "\n",
        "###**Number of Epochs**\n",
        "\n",
        "Sometimes, just increasing the number of epochs for model training\n",
        "delivers better results, although this comes at the cost of increased\n",
        "computation and training time.\n",
        "\n",
        "\n",
        "\n",
        "###**Weight Initialization**\n",
        "\n",
        "Initializing the weights for your network also has a tremendous impact\n",
        "on the overall performance. A good weight initialization technique not\n",
        "only speeds up the training process but also circumvents deadlocks in\n",
        "the model training process. By default, the Keras framework uses glorot\n",
        "uniform initialization, also called Xavier uniform initialization, but this can be changed as per your needs. We can initialize the weights for a layer\n",
        "using the kernel initializer parameter as well as bias using a bias initializer.\n",
        "Other popular options to select are ‘He Normal’ and ‘He Uniform’\n",
        "initialization and ‘lecun normal’ and ‘lecun uniform’ initialization.\n",
        "There are quite a few other options available in Keras too, but the\n",
        "aforementioned choices are the most popular.\n",
        "\n",
        "The following code snippet showcases an example of initializing\n",
        "weights in a layer of a DNN with random_uniform.\n",
        "\n",
        "*from keras import Sequential\n",
        "from keras.layers import Dense\n",
        "model = Sequential()\n",
        "model.add(Dense(64,activation=\"relu\", input_dim = 32, kernel_\n",
        "initializer = \"random_uniform\",bias_initializer = \"zeros\"))\n",
        "model.add(Dense(1,activation=\"sigmoid\"))*\n",
        "\n",
        "\n",
        "\n",
        "###**Batch Size**\n",
        "\n",
        "Using a moderate batch size always helps achieve a smoother learning\n",
        "process for the model. A batch size of 32 or 64, irrespective of the dataset\n",
        "size and the number of samples, will deliver a smooth learning curve in\n",
        "most cases. Even in scenarios where your hardware environment has\n",
        "large RAM memory to accommodate a bigger batch size, I would still\n",
        "recommend staying with a batch size of 32 or 64.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###**Learning Rate**\n",
        "\n",
        "Learning rate is defined in the context of the optimization algorithm. It\n",
        "defines the length of each step or, in simple terms, how large the updates\n",
        "to the weights in each iteration can be made. Throughout this book, we\n",
        "have ignored setting or changing the learning rate, as we have used the\n",
        "default values for the respective optimization algorithms, in our case\n",
        "Adam. The default value is 0.001, and this is a great choice for most scenarios. However, in some special cases, you might cross paths with a\n",
        "use case where it might be better to go with a lower learning rate or maybe\n",
        "slightly higher.\n",
        "\n",
        "\n",
        "\n",
        "###**Activation Function**\n",
        "\n",
        "We have a generous choice of activation functions for the neurons. In most\n",
        "cases, ReLU works perfectly. You could almost always go ahead with ReLU\n",
        "as an activation for any use case and get favorable results. In cases where\n",
        "ReLU might not be delivering great results, experimenting with PReLU is a\n",
        "great option.\n",
        "\n",
        "\n",
        "\n",
        "###**Optimization**\n",
        "\n",
        "Similar to activation functions, we also have a fairly generous number of\n",
        "choices available for the optimization algorithm of the network. While\n",
        "the most recommended is Adam, in scenarios where Adam might not\n",
        "be delivering the best results for your architecture, you could explore\n",
        "Adamax as well as Nadam optimizers. Adamax has mostly been a better\n",
        "choice for architectures that have sparsely updated parameters like word\n",
        "embeddings, which are mostly used in natural language processing\n",
        "techniques. We have not covered these advanced topics in the book, but it\n",
        "is good to keep these points in mind while exploring various architectures.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEf13DGYBAR4"
      },
      "source": [
        "#**Approaches for Hyperparameter Tuning**\n",
        "\n",
        "So far, we have discussed various hyperparameters that are available\n",
        "for our DL models and also studied the most recommended options for\n",
        "generic situations. However, selecting the most appropriate value for\n",
        "a hyperparameter based on the data and the type of problem is more\n",
        "of an art. The art is also arduous and painfully slow. The process of\n",
        "hyperparameter tuning in DL is almost always slow and resource intensive.\n",
        "However, based on the style of selecting a value for hyperparameter andfurther tuning model performance, we can roughly divide the different\n",
        "types of approaches into four broad categories:\n",
        "\n",
        "\n",
        "*    Manual Search\n",
        "\n",
        "*   Grid Search\n",
        "*  Random Search\n",
        "\n",
        "*   Bayesian Optimization\n",
        "\n",
        "\n",
        "Out of the four aforementioned approaches, we will have a brief look\n",
        "into the first three. Bayesian optimization is altogether a long and difficult\n",
        "topic that is beyond the scope for our book. Let’s have a brief look at the\n",
        "first three approaches.\n",
        "\n",
        "\n",
        ">\n",
        "###**Manual Search**\n",
        "\n",
        "Manual search, as the name implies, is a completely manual way of\n",
        "selecting the best candidate value for the desired hyperparameters\n",
        "in a DL model. This approach requires phenomenal experience in\n",
        "training networks to get the right set of candidate values for all desired\n",
        "hyperparameters using the least number of experiments. Often this\n",
        "approach is highly efficient, provided you have sound experience in using\n",
        "them. The best approach to start with manual search is simply to leverage\n",
        "all the recommended values for a given hyperparameter and then to start\n",
        "training the network. The results may not be the best, but would definitely\n",
        "not be the worst. It’s a good starting point for any newbie in the field to\n",
        "experiment with a few lowest-risk hyperparameter candidates.\n",
        "\n",
        "\n",
        "\n",
        "###**Grid Search**\n",
        "\n",
        "In the grid search approach, you literally experiment with all possible\n",
        "combinations for a defined set of values of a hyperparameter. The name “grid”\n",
        "is actually derived from the gridlike combinations for the provided values of\n",
        "each hyperparameter. The following is a sample view of how a logical grid\n",
        "would look for three hyperparameters with three distinct values in each.he grid search method in sklearn to accomplish the results. The following\n",
        "code snippet showcases the means to use grid search from the sklearn\n",
        "package by using the Keras wrapper for a dummy model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TMPvS8B8FHri",
        "outputId": "e5587256-ea57-4460-e430-9594eafe0508"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (<ipython-input-2-594a02c8ea65>, line 27)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-594a02c8ea65>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    grid_model = GridSearchCV(estimator=model, param_grid=param_\u001b[0m\n\u001b[0m                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ],
      "source": [
        "from keras import Sequential\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "#Generate dummy data for 3 features and 1000 samples\n",
        "x_train = np.random.random((1000, 3))\n",
        "#Generate dummy results for 1000 samples: 1 or 0\n",
        "y_train = np.random.randint(2, size=(1000, 1))\n",
        "#Create a python function that returns a compiled DNN model\n",
        "def create_dnn_model():\n",
        " model = Sequential()\n",
        " model.add(Dense(12, input_dim=3, activation='relu'))\n",
        " model.add(Dense(1, activation='sigmoid'))\n",
        " model.compile(loss='binary_crossentropy', optimizer='adam',\n",
        "metrics=['accuracy'])\n",
        " return model\n",
        "#Use Keras wrapper to package the model as an sklearn object\n",
        "model = KerasClassifier(build_fn=create_dnn_model)\n",
        "# define the grid search parameters\n",
        "batch_size = [32,64,128]\n",
        "epochs = [15, 30, 60]\n",
        "#Create a list with the parameters\n",
        "param_grid = {\"batch_size\":batch_size, \"epochs\":epochs}\n",
        "#Invoke the grid search method with the list of hyperparameters\n",
        "grid_model = GridSearchCV(estimator=model, param_grid=param_\n",
        "grid, n_jobs=-1)\n",
        "#Train the model\n",
        "grid_model.fit(x_train, y_train)\n",
        "#Extract the best model grid search\n",
        "best_model = grid_model.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Random Search**\n",
        "\n",
        "\n",
        "An improved alternative to grid search is random search. In a random\n",
        "search, rather than selecting a value for the hyperparameter from a defined\n",
        "list of numbers, like learning rate, we can instead choose randomly from a\n",
        "distribution. This is, however, only possible for numeric hyperparameters.\n",
        "So, instead of trying a learning rate of 0.1, 0.01, or 0.001, it can alternatively\n",
        "pick up a random value for learning rate from a distribution we define\n",
        "with some properties. The parameter now has a larger range of values\n",
        "to experiment with and also much higher chances of getting better\n",
        "performance. It overcomes the disadvantage of a human guessing the\n",
        "best value for the hyperparameter confined within the defined range\n",
        "by inducing randomness to bring the chance for better hyperparameter\n",
        "selection. In reality, for most practical cases, random search mostly\n",
        "outperforms grid search."
      ],
      "metadata": {
        "id": "ESJvDmKjxag8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Model Deployment**\n",
        "\n",
        "Now, we can finally discuss a few important pointers on model\n",
        "deployment. We started with learning Keras and DL, experimented with\n",
        "actual DNNs for regression and classification, and then discussed tuning\n",
        "hyperparameters for improved model performance. We can now discuss a\n",
        "few guidelines for deploying a DL model in a production environment.\n",
        "I want to clarify that we won’t actually be learning the process of deploying\n",
        "a model in production as a software engineer or discuss the DL software\n",
        "pipeline and architecture for a large enterprise project. We will instead\n",
        "focus on a couple of important aspects to be kept in mind while deploying\n",
        "the actual model."
      ],
      "metadata": {
        "id": "EltxXd15y4ov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Saving Models to Memory**\n",
        "\n",
        "Another useful point we didn’t discuss during the course of this chapter is\n",
        "saving the model as a file into memory and reusing it at some other point\n",
        "in time. The reason this becomes extremely important in DL is the time\n",
        "consumed in training large models. You shouldn’t be surprised when\n",
        "you encounter DL engineers who have been training models for weeks\n",
        "at a stretch on a supercomputer. Modern DL models that encompass\n",
        "image, audio, and unstructured text data consume a significant amount\n",
        "of time for training. A handy practice in such scenarios would be to have\n",
        "the ability to pause and resume training for a DL model and also save the\n",
        "intermediate results so that the training performed up to a certain point\n",
        "of time doesn’t go to waste. This can be achieved with a simple callback (a\n",
        "procedure in Keras that can be applied to the model at different stages of\n",
        "training) that would save the weights of the model to a file along with the\n",
        "model structure after a defined milestone.\n",
        "This saved model can later be\n",
        "imported again whenever you want to resume the training. The process\n",
        "continues just like you would want it to. All we need to do is take care of\n",
        "saving the model structure as well as the weights after an epoch or when\n",
        "we have the best model in place. Keras provides the ability to save models\n",
        "after every epoch or save the best model during training for multiple\n",
        "epochs.\n",
        "\n",
        "An example of saving the best weights of a model during training for a\n",
        "large number of epochs is shown in the following snippet."
      ],
      "metadata": {
        "id": "wcmTNiCUz24P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define the file path pattern for saving model weights\n",
        "# {epoch:.2f} will be replaced with the current epoch number\n",
        "# {val_acc:.2f} will be replaced with the validation accuracy at that epoch\n",
        "filepath = \"ModelWeights-{epoch:.2f}-{val_acc:.2f}.hdf5\"\n",
        "\n",
        "# Create a checkpoint callback to save the best model weights during training\n",
        "checkpoint = ModelCheckpoint(filepath, save_best_only=True, monitor=\"val_acc\")\n",
        "\n",
        "# Train the model using the defined checkpoint callback\n",
        "model.fit(x_train, y_train, callbacks=[checkpoint], epochs=100, batch_size=64)"
      ],
      "metadata": {
        "id": "t7esBjdK0QzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Alternatively, you can also save a model in its entire form after\n",
        "finishing training using the save_model method and later load it into\n",
        "memory (maybe the next day) using the load_model method. An example\n",
        "is shown in the following code snippet.*"
      ],
      "metadata": {
        "id": "B4KilW4V16z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "#Train a model for defined number of epochs\n",
        "model.fit(x_train, y_train, epochs=100, batch_size=64)\n",
        "# Saves the entire model into a file named as 'dnn_model.h5'\n",
        "model.save('dnn_model.h5')\n",
        "# Later, (maybe another day), you can load the trained model for prediction.\n",
        "model = load_model('dnn_model.h5')"
      ],
      "metadata": {
        "id": "Kw49TVY82Oqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Retraining the Models with New Data**\n",
        "\n",
        "When you deploy your model into production, the ecosystem will continue\n",
        "to generate more data, which can be used for training your models again.\n",
        "Say, for the credit card fraud use case, you trained your model with 100K\n",
        "samples and got a performance of 93% accuracy. You feel the performance\n",
        "is good enough to get started, so you deploy your model into production.\n",
        "Over a period of one month, an additional 10K samples are available from\n",
        "the new transactions made by customers. Now you would want your model\n",
        "to leverage this newly available data and improve its performance even further. To achieve this, you don’t need to retrain the entire model again;\n",
        "you could instead use the pause-and-resume approach. All you need to do\n",
        "is use the weights of the model already trained and provide additional data\n",
        "with a few epochs to pass and iterate over the new samples. The weights\n",
        "it has already learned don’t need to be disposed; you can simply use the\n",
        "pause-and-resume formula and continue with the incremental data"
      ],
      "metadata": {
        "id": "dKYB9Pdz2ziM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Online Models**\n",
        "\n",
        "An immediate question you may ponder after understanding the process\n",
        "of retraining the model is how frequently should you do this: is it a\n",
        "good approach to retrain every day, every week, or every month? The\n",
        "right answer is to retrain as frequently as you want. There is no harm\n",
        "in incrementally training your models every time a new data point is\n",
        "available as long as the computation required is not a bottleneck. A good\n",
        "practice would be to iterate a training instance as soon as a new batch\n",
        "of samples is available. So, if you have set a batch_size of 64, you could\n",
        "automate the model training to ingest the newly available batch of data\n",
        "and further improve performance on future predictions by automating\n",
        "the software infrastructure to train the model for every new batch of data\n",
        "samples. An extremely aggressive way to keep the model performance at\n",
        "the best would be to incrementally train with every new data point and\n",
        "add previous samples as the remainder of the batch. This approach is\n",
        "extremely computation intensive and also less rewarding. This approach of\n",
        "becoming ultra-real-time and incrementally training for every new sample\n",
        "instead of a batch is usually not recommended.\n",
        "Such models, which are always learning as and when a new batch of\n",
        "data is available, are called online models. The most popular examples\n",
        "of online models can be seen on your phone. Features like predictive text\n",
        "and autocorrect improve dramatically over time. If you generally type in\n",
        "a specific style, say combining two languages or shortening few words or\n",
        "using slang and so on, you will notice that the mobile phone quite actively  An extremely aggressive way to keep the model performance at\n",
        "the best would be to incrementally train with every new data point and\n",
        "add previous samples as the remainder of the batch. This approach is\n",
        "extremely computation intensive and also less rewarding. This approach of\n",
        "becoming ultra-real-time and incrementally training for every new sample\n",
        "instead of a batch is usually not recommended.\n",
        "Such models, which are always learning as and when a new batch of\n",
        "data is available, are called online models. The most popular examples\n",
        "of online models can be seen on your phone. Features like predictive text\n",
        "and autocorrect improve dramatically over time. If you generally type in\n",
        "a specific style, say combining two languages or shortening few words or\n",
        "using slang and so on, you will notice that the mobile phone quite actively"
      ],
      "metadata": {
        "id": "CTzeL-kT3ZMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Delivering Your Model As an API**\n",
        "\n",
        "The best practice today in delivering your model as a service to a larger\n",
        "software stack is by delivering it as an API. This is extremely useful and\n",
        "effective, as it completely gets rid of the tech-stack requirements. Your\n",
        "model can easily collaborate between a diverse and complex set of\n",
        "components in a software ecosystem where you can worry less about the\n",
        "language or framework you used to develop the model. While Python and Keras are almost universal in today’s modern\n",
        "tech stack, we can still expect a few exceptions where this choice might\n",
        "not be an easy option to integrate. Therefore, we can always choose API\n",
        "as the preferred mode of deployment for a DL model and define the\n",
        "requirements for data and calling style of the API appropriately.\n",
        "There are two extremely useful and easy-to-operate options for\n",
        "deploying your service as an API. You could either use Flask (a lightweight\n",
        "Python web framework) or Amazon Sagemaker (available on AWS). There\n",
        "are other options too, and I encourage you to explore them. There is an\n",
        "extremely well-written article on Keras Blogs on deploying your DL model\n",
        "using Flask."
      ],
      "metadata": {
        "id": "N_uwNCXK5Xq7"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaoh+GQM3tf54dNl690428",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}